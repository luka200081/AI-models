Great, I’ll gather detailed information on how code generation models are trained to understand prompts. This will include training data sources, architectures used, pretraining and fine-tuning methods, and how these models are aligned to follow developer-style natural language prompts.


# Training Data for Code Models

Code-generation LLMs are trained on massive corpora of source code (and often code-related text).  For example, OpenAI Codex was fine-tuned on **159 GB of Python code** scraped from 54 million public GitHub repos, and powers GitHub Copilot.  Meta’s Code Llama was trained on a **“code-heavy”** dataset – about **500B tokens** of de-duplicated public code – together with a small (≈8%) fraction of natural-language text about code (e.g. Q\&A threads, documentation) to help it understand developer queries.  The BigCode *StarCoder* family uses the *The Stack* datasets: StarCoder2’s pre-training corpus (“The Stack v2”) is drawn from the Software Heritage archive and other code sources (GitHub PRs, Kaggle notebooks, etc.), yielding \~**900 B tokens (\~3 TB)** of code.  Newer models like DeepSeek-Coder were trained from scratch on **2 trillion tokens (∼87% code, 13% natural-language)** covering dozens of languages.  In summary, code models ingest vast amounts of **open-source code (often filtered and deduplicated)** and related text (comments, Q\&A) as their raw training data.

# Pretraining Strategies

Most code LLMs use **autogressive language modeling** over code tokens as their primary objective.  For instance, Codex and StarCoder are GPT-style models predicting the next token in source code.  Some models add specialized tasks: Code Llama’s training for 7B/13B/70B models includes a **“causal infilling”** objective, where random code spans are masked out and the model predicts the missing part (via a variant of span prediction).  DeepSeek-Coder’s training similarly includes a “fill-in-the-blank” task over code with a very long (16K token) context window.  In contrast, models like CodeBERT use **masked-token prediction** (MLM) on code, but these are typically **not used for generation** – they’re more for code understanding.  Overall, code LLMs adopt the same Transformer-based objectives as NLP models: primarily next-token prediction on code text, sometimes augmented with span/masked infilling to improve code completion.

# Instruction Tuning and Alignment

To make code models follow developer prompts, most undergo an additional supervised or reinforcement-based tuning phase.  For example, early Codex variants were simply fine-tuned on code (the original paper notes “Codex, a GPT model fine-tuned on public GitHub code”).  More recent systems use **instruction tuning** or **RLHF**.  Meta’s Code Llama – in its *Instruct* variant – is fine-tuned on two kinds of data: first, thousands of human-generated Q\&A and dialogue examples (the Llama 2 “RLHF v5” dataset) to instill general helpfulness and safety; second, a large synthetic code-instruction dataset.  The latter is created via “self-instruct”: programming questions generated by Llama 2, with unit tests and solutions produced by Code Llama itself, yielding \~14k high-quality (question, test, answer) triples.  OpenAI’s newest Codex (codex-1) is **trained with reinforcement learning** on coding tasks, optimizing for human-like style and correctness.  DeepSeek’s code models are first pretrained as “base” and then **fine-tuned on 2B tokens of instruction data** to create DeepSeek-Coder-Instruct models.  Even StarCoder2 has an instruct-tuned version: a fully open *self-alignment* pipeline where StarCoder2-15B generates thousands of (instruction→code) examples from its own training corpus, then fine-tunes on them.  In short, after the initial code-language modeling, these models often see **explicit instruction-finetuning** (via supervised pairs or RLHF) so they learn to follow prompts.

# Prompt Structure and Formatting

Code LLMs are sensitive to how a prompt is formatted.  Commonly, a natural-language description (or docstring) is prepended to a function signature or code context.  In the HumanEval benchmark, for example, each prompt is structured as a header + function signature + docstring, and the model is trained to generate the function body.  This guides the model to treat the developer’s text as instructions.  Special **stop tokens** (e.g. newline, new function or class keywords) are also used at generation time to truncate output cleanly.  Models like Code Llama can handle extremely long contexts (after a fine-tuning stage) – up to **100K tokens** – enabling them to digest entire code files or repos in one prompt.  Conversely, if a prompt is poorly formatted (e.g. missing a docstring or using ambiguous terminology), even a powerful model may produce lower-quality code.  In practice, developers often include comments, inline hints, or partial code; models leverage those cues if seen during training.  Thus, prompt structure (docstrings, function headers, examples) strongly influences the model’s output.

# Learning Prompt-Following: Intrinsic vs. Aligned

Transformer code models **by default learn surface patterns** from their training data, not “understanding” intents.  If the pretraining corpus contains code solutions following docstrings or comments, the model may *implicitly* learn to map prompts to code.  Indeed, GPT-3 already generated simple code from textual prompts even without fine-tuning.  However, pure language-model training alone often isn’t enough for reliable instruction-following.  That’s why many teams apply **supervised fine-tuning and RLHF**.  For example, Code Llama Instruct inherits Llama 2’s RLHF-based dialogue data, which gives it a natural tendency to follow commands.  OpenAI explicitly used RL (rewarding tests passing, etc.) when building Codex to “adhere precisely to instructions”.  In contrast, a model that is only trained autoregressively on code (like a vanilla StarCoder without instruct tuning) will generate plausible continuations but may not reliably heed a user’s “do X” prompt in natural language.  Summarizing: **natural-language understanding in code LMs is mostly learned through the combination of (a) exposure to code with comments in training and (b) explicit instruction/RL tuning** (rather than arising spontaneously).

# Notable Model Examples

* **OpenAI Codex / GitHub Copilot**: Early Codex models were GPT-3 derivatives fine-tuned on GitHub code.  Their prompts were Python-centric (docstrings→functions) and performance scaled with model size.  In 2025, OpenAI announced *codex-1*, an o3 variant “trained using reinforcement learning on real-world coding tasks” to better follow developer instructions.  Copilot (the product) uses Codex internally, supplying completion suggestions to programmers.
* **Meta Code Llama**: Starting from Llama 2, Code Llama is pretrained on \~500B tokens of code and some code-related text.  Its architecture supports code infilling and *long contexts* (projects).  Code Llama-Instruct models are then fine-tuned on the Llama2 RLHF conversation data plus a self-generated programming Q\&A dataset.  This yields a model that is strong at both understanding “how to code X?” prompts and generating safe, helpful explanations or code.
* **BigCode StarCoder / StarCoder2**: StarCoder (15.5B) was trained on permissively-licensed code (The Stack v1). StarCoder2 (up to 15B) uses a **4× larger** corpus (The Stack v2, \~3 TB of code) drawn from Software Heritage, GitHub PRs, etc.  It is trained as a standard autoregressive code model (3–4 trillion tokens).  A separate “StarCoder2-Instruct” variant uses a self-alignment pipeline (model-generated Q→A pairs) to teach it to follow instructions.  These open models achieve state-of-art in many benchmarks.
* **DeepSeek-Coder**: A series of models (1B–33B) trained **from scratch** on 2T tokens of multilingual code (87% code, 13% English/Chinese).  The base models use a long (16K token) context and a fill-in-the-blank objective.  They are then fine-tuned on \~2B tokens of instruction data to create “Instruct” versions.  DeepSeek reports that its 33B base model outperforms other open models on Python benchmarks, and the instruct-tuned 33B rivals GPT-3.5 on some tasks.
* **Google DeepMind AlphaCode**: A research system targeting competitive programming.  AlphaCode uses transformer models pretrained on selected public code, then **fine-tunes on a curated set of programming problems**.  At inference, it generates *massive sample sets* of C++/Python solutions and filters them, rather than relying on a single deterministic output.  The approach reached roughly “median competitor” performance on Codeforces contests.  While AlphaCode’s architecture is proprietary, its training pipeline illustrates fine-tuning on domain-specific problem-solution pairs plus clever sampling.

**Sources:** Peer-reviewed papers and official reports on code LLMs.  (Citations link to specific model and training details.)
